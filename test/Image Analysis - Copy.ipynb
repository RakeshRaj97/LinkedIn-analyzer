{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "from PIL import Image\n",
    "# import simplejson as json \n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "import time\n",
    "from datetime import date\n",
    "import http.client, urllib.request \n",
    "import urllib.parse, urllib.error \n",
    "import base64, sys \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign in Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signin(file='config.txt'):\n",
    "\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "    #Login credentials for Linkedin\n",
    "    file=open(file)\n",
    "    lines=file.readlines()\n",
    "    username=lines[0]\n",
    "    password=lines[1]\n",
    "\n",
    "    # locate username and password form login page by HTML id\n",
    "    enter_username = driver.find_element_by_id('username')\n",
    "    enter_password=driver.find_element_by_id('password')\n",
    "\n",
    "    # send_keys() to simulate key strokes\n",
    "    enter_username.send_keys(username)\n",
    "    enter_password.send_keys(password)\n",
    "\n",
    "    #Submit Credentials\n",
    "    enter_password.submit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scroll Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll():\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    \n",
    "    #Scroll to see more and click if visible\n",
    "    driver.execute_script(\"window.scrollTo(0,560);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    \n",
    "    try:\n",
    "        driver.find_element_by_id('line-clamp-show-more-button').click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Scroll back to top \n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0,0);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcitions for Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_name():\n",
    "    name_all= soup.find('div',{'class':'flex-1 mr5'})#\n",
    "    name=name_all.find('li',{'class':'inline t-24 t-black t-normal break-words'}).get_text().strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_image():\n",
    "    imgtag=soup.select('img[title]')\n",
    "    urllib.request.urlretrieve(imgtag[0]['src'],'scraped_image.jpg')\n",
    "    return imgtag[0]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_about():\n",
    "    try:\n",
    "        about_all= soup.find('span',{'class':'lt-line-clamp__raw-line'}).get_text().strip()\n",
    "        return about_all\n",
    "    except:\n",
    "        return 'None'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_header():\n",
    "    name_all= soup.find('div',{'class':'flex-1 mr5'})\n",
    "    header=name_all.find_all('h2')[0].get_text().strip()\n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_location():\n",
    "    name_all= soup.find('div',{'class':'flex-1 mr5'})\n",
    "    location=name_all.find('li',{'class':'t-16 t-black t-normal inline-block'}).get_text().strip()\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_connections():\n",
    "    name_all= soup.find('div',{'class':'flex-1 mr5'})\n",
    "    try:\n",
    "        connections=name_all.find('span',{'class':'t-16 t-bold link-without-visited-state'}).get_text().strip()\n",
    "        return connections\n",
    "    except:\n",
    "        pass\n",
    "    connections=name_all.find('span',{'class':'t-16 t-black t-normal'}).get_text().strip()\n",
    "    return connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_joining():\n",
    "    joining='Present'\n",
    "    \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__company-summary-info'})\n",
    "        experience=company_all.find_all('h4')[0].get_text().strip().split('\\n')[1]\n",
    "        return joining\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__summary-info pv-entity__summary-info--background-section'})\n",
    "        joining=company_all.find_all('span')[2].get_text().strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_experience():\n",
    "    experience='Unknown'\n",
    "    \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__company-summary-info'})\n",
    "        experience=company_all.find_all('h4')[0].get_text().strip().split('\\n')[1]\n",
    "        return experience\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__summary-info pv-entity__summary-info--background-section'})\n",
    "        experience=company_all.find_all('span')[4].get_text().strip()    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return experience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unknown']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_company():\n",
    "    company='Unknown'\n",
    "    \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__company-summary-info'})\n",
    "        company=company_all.find_all('h3')[0].get_text().strip().split('\\n')[1]\n",
    "        return company\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__summary-info pv-entity__summary-info--background-section'})\n",
    "        company=company_all.find_all('p')[1].get_text().strip().split('\\n')[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_title():\n",
    "    title='Unknown'\n",
    "    \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__summary-info-v2 pv-entity__summary-info--background-section pv-entity__summary-info-margin-top mb2'})\n",
    "        title=company_all.find_all('h3')[0].get_text().strip().split('\\n')[1]\n",
    "        return title\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        company_all= soup.find('div',{'class':'pv-entity__summary-info pv-entity__summary-info--background-section'})\n",
    "        title=company_all.find_all('h3')[0].get_text().strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_college():\n",
    "    college_all= soup.find('div',{'class':'pv-entity__degree-info'})\n",
    "    college=college_all.find_all('h3')[0].get_text().strip()\n",
    "    return college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_degree():\n",
    "    college_all= soup.find('div',{'class':'pv-entity__degree-info'})\n",
    "    degree=college_all.find_all('span')[1].get_text().strip()\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_degree_year():\n",
    "    college_all= soup.find_all('time')\n",
    "    year1=college_all[0].get_text().strip()\n",
    "    year2=college_all[1].get_text().strip()\n",
    "    year=year1 +'-'+ year2\n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Linkedin and enter profiles to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('Link.csv')\n",
    "\n",
    "dates=[]\n",
    "name=[]\n",
    "image=[]\n",
    "about=[]\n",
    "header=[]\n",
    "location=[]\n",
    "connections=[]\n",
    "joining=[]\n",
    "experience=[]\n",
    "company=[]\n",
    "title=[]\n",
    "college=[]\n",
    "degree=[]\n",
    "degree_year=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-206-bf7595d1e3bf>:8: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  dates.append(pd.datetime.today().strftime('%d %b, %Y'))\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "signin()\n",
    "for link in df1.URL.values:\n",
    "    Linkedin_url = link\n",
    "    driver.get(Linkedin_url)\n",
    "    soup=scroll()\n",
    "    dates.append(pd.datetime.today().strftime('%d %b, %Y'))\n",
    "    \n",
    "    #Scrapping data and appending it to lists\n",
    "    name.append(Scrape_name())\n",
    "    image.append(Scrape_image())\n",
    "    about.append(Scrape_about())\n",
    "    header.append(Scrape_header())\n",
    "    location.append(Scrape_location())\n",
    "    connections.append(Scrape_connections())\n",
    "    joining.append(Scrape_joining())\n",
    "    experience.append(Scrape_experience())\n",
    "    company.append(Scrape_company())\n",
    "    title.append(Scrape_title())\n",
    "    college.append(Scrape_college())\n",
    "    degree.append(Scrape_degree())\n",
    "    degree_year.append(Scrape_degree_year())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>image</th>\n",
       "      <th>about</th>\n",
       "      <th>header</th>\n",
       "      <th>location</th>\n",
       "      <th>connections</th>\n",
       "      <th>joining</th>\n",
       "      <th>experience</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>college</th>\n",
       "      <th>degree</th>\n",
       "      <th>degree_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19 Aug, 2020</td>\n",
       "      <td>Shweta Sharma</td>\n",
       "      <td>https://media-exp1.licdn.com/dms/image/C5603AQ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Data Science Consultant  at Arcadis</td>\n",
       "      <td>Clayton, Victoria, Australia</td>\n",
       "      <td>500+ connections</td>\n",
       "      <td>Nov 2019 – Present</td>\n",
       "      <td>10 mos</td>\n",
       "      <td>Arcadis</td>\n",
       "      <td>Data Science Consultant</td>\n",
       "      <td>Monash University</td>\n",
       "      <td>Master of Data Science</td>\n",
       "      <td>2018-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19 Aug, 2020</td>\n",
       "      <td>Rohan Singh</td>\n",
       "      <td>https://media-exp1.licdn.com/dms/image/C5635AQ...</td>\n",
       "      <td>Recently completed Masters of Data Science fro...</td>\n",
       "      <td>Data Scientist Intern at AI Australia</td>\n",
       "      <td>Malvern East, Victoria, Australia</td>\n",
       "      <td>385 connections</td>\n",
       "      <td>Aug 2020 – Present</td>\n",
       "      <td>1 mo</td>\n",
       "      <td>AI Australia</td>\n",
       "      <td>Data Scientist Intern</td>\n",
       "      <td>Monash University</td>\n",
       "      <td>Master of Data Science</td>\n",
       "      <td>2018-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19 Aug, 2020</td>\n",
       "      <td>Cody Middlebrook</td>\n",
       "      <td>https://media-exp1.licdn.com/dms/image/C5103AQ...</td>\n",
       "      <td>Most quickly identified by my Filmmaking, doub...</td>\n",
       "      <td>The AI Guy | Reimagining Artificial Intelligen...</td>\n",
       "      <td>Sydney, Australia</td>\n",
       "      <td>500+ connections</td>\n",
       "      <td>Present</td>\n",
       "      <td>1 yr 7 mos</td>\n",
       "      <td>AI Films</td>\n",
       "      <td>Interim CEO</td>\n",
       "      <td>London Business School</td>\n",
       "      <td>Diploma in Computer Programming</td>\n",
       "      <td>1997-2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date              name  \\\n",
       "0  19 Aug, 2020     Shweta Sharma   \n",
       "1  19 Aug, 2020       Rohan Singh   \n",
       "2  19 Aug, 2020  Cody Middlebrook   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://media-exp1.licdn.com/dms/image/C5603AQ...   \n",
       "1  https://media-exp1.licdn.com/dms/image/C5635AQ...   \n",
       "2  https://media-exp1.licdn.com/dms/image/C5103AQ...   \n",
       "\n",
       "                                               about  \\\n",
       "0                                               None   \n",
       "1  Recently completed Masters of Data Science fro...   \n",
       "2  Most quickly identified by my Filmmaking, doub...   \n",
       "\n",
       "                                              header  \\\n",
       "0                Data Science Consultant  at Arcadis   \n",
       "1              Data Scientist Intern at AI Australia   \n",
       "2  The AI Guy | Reimagining Artificial Intelligen...   \n",
       "\n",
       "                            location       connections             joining  \\\n",
       "0       Clayton, Victoria, Australia  500+ connections  Nov 2019 – Present   \n",
       "1  Malvern East, Victoria, Australia   385 connections  Aug 2020 – Present   \n",
       "2                  Sydney, Australia  500+ connections             Present   \n",
       "\n",
       "   experience       company                    title                 college  \\\n",
       "0      10 mos       Arcadis  Data Science Consultant       Monash University   \n",
       "1        1 mo  AI Australia    Data Scientist Intern       Monash University   \n",
       "2  1 yr 7 mos      AI Films              Interim CEO  London Business School   \n",
       "\n",
       "                            degree degree_year  \n",
       "0           Master of Data Science   2018-2020  \n",
       "1           Master of Data Science   2018-2020  \n",
       "2  Diploma in Computer Programming   1997-2000  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "linkedin = {'date':dates,'name':name,'image':image,'about':about,'header':header,'location':location,'connections':connections,\\\n",
    "            'joining':joining,'experience':experience,'company':company,'title':title,'college':college,'degree':degree,\\\n",
    "            'degree_year':degree_year}  \n",
    "    \n",
    "linkedin_df = pd.DataFrame(linkedin) \n",
    "linkedin_df.to_csv('Linkedin_profiles.csv')\n",
    "linkedin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
